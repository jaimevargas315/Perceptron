## Problem 7
```julia
using LinearAlgebra
using Plots
using Random

## Newton’s Method on a Quadratic Cost Function: J(w) = 0.5 * (w₁² + 5w₂²)

# --- 1. Quadratic Cost Function (J(w)) ---

function quadratic_cost(w::Vector{Float64})::Float64
    """ J(w) = 0.5 * (w₁² + 5w₂²) """
    w1, w2 = w[1], w[2]
    return 0.5 * (w1^2 + 5.0 * w2^2)
end

# --- 2. Gradient Vector (g) ---

function quadratic_gradient(w::Vector{Float64})::Vector{Float64}
    """ g = ∇J(w) = [w₁, 5w₂]ᵀ """
    return [w[1], 5.0 * w[2]]
end

# --- 3. Constant Hessian and Inverse (H and H⁻¹) ---

# Hessian H is constant: [[1, 0], [0, 5]]
const HESSIAN_INV = [
    1.0 0.0; 
    0.0 0.2 # 1/5 = 0.2
]

# --- 4. Newton's Method Algorithm ---

function newtons_method(
    initial_w::Vector{Float64};
    η::Float64 = 1.0,           # Learning rate (fixed at 1.0)
    max_iterations::Integer = 1
)::Tuple{Vector{Vector{Float64}}, Vector{Float64}}
    """
    Implements Newton's Method: w ← w - η * H⁻¹ * g
    For a quadratic function with η=1, convergence should occur in 1 step.
    """
    
    w = copy(initial_w)
    weight_history = Vector{Float64}[]
    cost_history = Float64[]
    push!(weight_history, copy(w)) 
    push!(cost_history, quadratic_cost(w))
    
    # Run loop for max_iterations (should be 1 for η=1)
    for k in 1:max_iterations
        g = quadratic_gradient(w)
        
        # Newton Step: Δw = -H⁻¹ * g
        delta_w = -HESSIAN_INV * g
        
        # Update rule: w ← w + η * Δw
        w += η * delta_w
        
        push!(weight_history, copy(w))
        push!(cost_history, quadratic_cost(w))
        
        # Optional: break if cost is near zero (convergence)
        if quadratic_cost(w) < 1e-12
            # println("Convergence to minimum achieved at step $k.")
            break
        end
    end
    
    return weight_history, cost_history
end


# --- 5. Plotting Function (Contour & Multiple Trajectories) ---

function plot_newtons_trajectories(all_w_histories::Vector{Vector{Vector{Float64}}})
    """ Plots the quadratic cost contour and all Newton's Method trajectories. """
    
    # Optimal minimum is at w* = [0, 0]
    w_star = [0.0, 0.0]
    
    range_lim = 0.5
    w_range = range(-range_lim, stop=range_lim, length=100)
    
    # Calculate the cost surface
    J_surface = [quadratic_cost([w1, w2]) for w2 in w_range, w1 in w_range]

    # Create the contour plot
    p = contour(w_range, w_range, J_surface, 
                fill=true, 
                levels=range(0.001, 0.5, length=15), 
                cbar=false, 
                title="Newton's Method (50 Runs, η=1.0)",
                xlabel="w₁", 
                ylabel="w₂",
                legend=:topright,
                colormap=:plasma) 

    # Plot the Optimal Solution w*
    scatter!(p, [w_star[1]], [w_star[2]], 
             label="Optimal w*", 
             marker=:star, markersize=10, markercolor=:red, markerstrokecolor=:black)
             
    all_final_w1 = Float64[]
    all_final_w2 = Float64[]

    # Plot all 50 trajectories
    for w_history in all_w_histories
        w1_path = [w[1] for w in w_history]
        w2_path = [w[2] for w in w_history]
        
        # Plot the single path (start to end)
        plot!(p, w1_path, w2_path, 
              label=nothing, 
              color=:lime, 
              linewidth=1.5, 
              alpha=0.6,
              markershape=:circle,
              markersize=3)

        # Collect final point
        if length(w_history) > 1
            push!(all_final_w1, w1_path[end])
            push!(all_final_w2, w2_path[end])
        end
    end
    
    # Plot final convergence points (should all be on top of w*)
    scatter!(p, all_final_w1, all_final_w2, 
             label="Final Weights (50)", 
             markersize=5, markershape=:xcross, markercolor=:purple, markerstrokecolor=:black)

    display(p)
end


# --- 6. Execution (50 Runs) ---
const NUM_RUNS = 50
eta_val = 1.0 
iterations = 1 # We only need one iteration for convergence

Random.seed!(42) # Set seed for reproducibility

all_w_histories = Vector{Vector{Float64}}[]
final_costs = Float64[]

println("--- Starting 50 Newton's Method Runs (η=1.0) ---")

# Run 50 experiments
for i in 1:NUM_RUNS
    # Gaussian initialization: μ=0, σ=0.1 -> randn(2) * 0.1
    initial_w = randn(2) * 0.1 
    
    (w_history, j_history) = newtons_method(
        initial_w; 
        η=eta_val, 
        max_iterations=iterations
    )
    
    push!(all_w_histories, w_history)
    push!(final_costs, j_history[end])
end

# Generate Plot showing all 50 trajectories
plot_newtons_trajectories(all_w_histories)

# --- Discussion Metrics ---
mean_final_cost = mean(final_costs)
println("\n--- Newton's Method Results (50 Runs) ---")
println("Mean Final Cost: J(w_final) = $(round(mean_final_cost, digits=18))")
println("Smallest Initial Cost: $(round(quadratic_cost(all_w_histories[1][1]), digits=4))")
println("Initial Weights for Example Run: $(round.(all_w_histories[1][1], digits=4))")
println("Final Weights for Example Run: $(round.(all_w_histories[1][end], digits=18))")
println("The results confirm that Newton's Method converges to the optimal weights w⋆ in a single step due to the quadratic nature of the cost function and the fixed learning rate η=1.0.")
```