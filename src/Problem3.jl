using LinearAlgebra

function trainBatchPerceptron(X::Vector{Vector{Float64}}, ğ::Vector{Float64}, Î·::Float64; ğ°=nothing, maxIter::Integer=50, tol=1e-9)
    if ğ°===nothing; ğ° = randn(length(X[1])+1); end
    iter = 0
    N = length(X)
    for outer iter âˆˆ 1:maxIter
        ğ°_old = copy(ğ°) #save centers to check for convergence

        # Initialize the total correction vector for this batch (epoch)
        # This vector accumulates sum_{n | x_n âˆˆ M} e_n * x_n
        Î”ğ°_batch = zeros(length(ğ°))
        
        # Loop through all training examples (the batch)
        for n âˆˆ 1:N
            
            # Augment x_n (the input vector) for calculations
            x_aug = [1.0; X[n]]
            
            # Compute the weighted sum and activation output
            nu = dot(ğ°, x_aug)
            y = sign(nu)
            
            # Compute the error: e = desired - actual
            e = ğ[n] - y
            
            # If misclassified (error is non-zero)
            if abs(e) > 1e-12 
                # Accumulate the correction term (e_n * x_n)
                # This performs: Î”ğ°_batch += e * x_aug
                Î”ğ°_batch .+= e * x_aug 
            end
        end
        
        # APPLY THE BATCH UPDATE: w <- w + Î· * Î”ğ°_batch
        # This step is performed ONLY ONCE after the entire dataset (batch) is processed
        ğ° .+= Î· * Î”ğ°_batch

        if norm(ğ°-ğ°_old) < tol #check for convergence
            break
        end
    end
    return ğ°::Vector{Float64}, iter::Integer
end
